{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRN3DiLafTF9",
        "outputId": "de6de4d2-70e4-4cb9-fe08-151cfbfbd0b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Collecting openai\n",
            "  Downloading openai-1.66.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Downloading openai-1.66.3-py3-none-any.whl (567 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m567.4/567.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.61.1\n",
            "    Uninstalling openai-1.61.1:\n",
            "      Successfully uninstalled openai-1.61.1\n",
            "Successfully installed openai-1.66.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C5P0dmRg4CO",
        "outputId": "c2ea1e72-5aaf-4233-c3a9-4446206c4c03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The ocean waves, they ebb and flow,\n",
            "The restless tides that come and go.\n",
            "A vast expanse of blue and green,\n",
            "A world unseen, a mystic dream.\n",
            "\n",
            "The salt air fills my lungs with peace,\n",
            "As seabirds soar and never cease.\n",
            "The sun reflects off waters deep,\n",
            "A shimmering veil, a promise to keep.\n",
            "\n",
            "Beneath the surface, a world untold,\n",
            "Where creatures swim in a dance so bold.\n",
            "Majestic whales and tiny fish,\n",
            "In harmony, they fulfill their wish.\n",
            "\n",
            "The ocean's beauty, wild and free,\n",
            "A timeless wonder for all to see.\n",
            "Its power, its mystery, will forever inspire,\n",
            "As we stand in awe of its infinite empire.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "# openai.api_key = \"sk-...q7kG\"\n",
        "\n",
        "\n",
        "client = openai.Client(api_key=\"Your openAI api key\")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Write a short poem about the ocean.\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2BgXzGPo1Ja",
        "outputId": "cdc455fd-e2a4-48cc-b264-b47b7e772f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://oaidalleapiprodscus.blob.core.windows.net/private/org-Ph2nVWgJ3GyDQ6peiNxJEJMC/user-VsFTafqQ85RSHKyZlOeNud4l/img-loVK6Tr2JnWKTF4FPg7XqIyc.png?st=2025-03-17T00%3A50%3A30Z&se=2025-03-17T02%3A50%3A30Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=d505667d-d6c1-4a0a-bac7-5c84a87759f8&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-03-16T17%3A57%3A10Z&ske=2025-03-17T17%3A57%3A10Z&sks=b&skv=2024-08-04&sig=RnAwareXbScRZxs22/OgfuOAQuWZDLQYk9S2VyDMyEA%3D\n"
          ]
        }
      ],
      "source": [
        "response = client.images.generate(\n",
        "    model=\"dall-e-2\",  # Use \"dall-e-3\" for higher quality\n",
        "    prompt=\"I'm skiing in Seven Springs in pink snow suits\",\n",
        "    n=1,\n",
        "    size=\"512x512\"\n",
        ")\n",
        "\n",
        "print(response.data[0].url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwOnVhXXkMam"
      },
      "source": [
        "üîπ 1. Text Summarization Bot (Easiest)\n",
        "Goal: Create a chatbot that summarizes long text using OpenAI‚Äôs API.\n",
        "What You‚Äôll Learn: API calls, prompt engineering, token limits, summarization techniques.\n",
        "Tech Needed: Google Colab, OpenAI API.\n",
        "Next Steps:\n",
        "Use gpt-3.5-turbo or gpt-4-turbo to summarize articles.\n",
        "Try different prompt strategies to improve summary quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1mtqCfFzRZ4",
        "outputId": "de4b1591-4310-4b57-c486-7ba95dc16997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary: The field of Artificial Intelligence (AI) has seen significant growth recently,\n",
            "with researchers and companies developing advanced technologies that can perform\n",
            "tasks previously done by humans. AI has applications in healthcare, finance, and\n",
            "entertainment, allowing machines to analyze data, recognize patterns, and make\n",
            "decisions.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import textwrap\n",
        "\n",
        "\n",
        "client = openai.Client(api_key=\"Your openAI api key\")\n",
        "\n",
        "def summarize_text(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",  # You can use \"gpt-4\" for better results\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # System message sets context\n",
        "            {\"role\": \"user\", \"content\": f\"Summarize the following text:\\n\\n{text}\"}  # User's request\n",
        "        ],\n",
        "\n",
        "        # Limit the number of tokens (reduce this value for even shorter summaries)\n",
        "        max_tokens=100,  # Limit the number of tokens for the summary\n",
        "        temperature=0.5  # Control creativity: 0.7 is a good balance\n",
        "        # Lower temperature can make the model more focused and concise\n",
        "    )\n",
        "    summary = response.choices[0].message.content.strip()\n",
        "    # Format the summary into lines of a fixed length\n",
        "    wrapped_summary = textwrap.fill(summary, width=80)  # Change the width as needed\n",
        "\n",
        "    return wrapped_summary\n",
        "\n",
        "    # return summary\n",
        "\n",
        "# Example usage\n",
        "sample_text = \"\"\"\n",
        "The field of Artificial Intelligence (AI) has grown significantly in recent years.\n",
        "Researchers and companies have been developing advanced AI technologies that can\n",
        "perform tasks that once required human intelligence. These technologies have applications\n",
        "in various fields, including healthcare, finance, and entertainment. With AI, machines can\n",
        "analyze large amounts of data, recognize patterns, and even make decisions.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarize_text(sample_text)\n",
        "print(\"Summary:\", summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40C2DId8kZEx"
      },
      "source": [
        "üîπ 2. AI-Powered Q&A Bot (Intermediate)\n",
        "Goal: A chatbot that answers domain-specific questions (e.g., about programming, history, or science).\n",
        "What You‚Äôll Learn: Embeddings (vector search), retrieval-augmented generation (RAG).\n",
        "Tech Needed: OpenAI API, Google Colab, Pinecone (optional for embeddings).\n",
        "Next Steps:\n",
        "Use gpt-4-turbo for answering questions.\n",
        "Store common Q&A pairs for quick responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4AunrcgjtDW"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import textwrap\n",
        "\n",
        "client = openai.Client(api_key=\"\")\n",
        "def ask_question(question):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": question}],\n",
        "        temperature=0.7,  # Adjust creativity\n",
        "        max_tokens=100    # Control response length\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "user_question = \"What is the capital of France? What is your name\"\n",
        "answer = ask_question(user_question)\n",
        "print(\"AI Answer:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfwDQjiOl05T"
      },
      "outputs": [],
      "source": [
        "# Enhance with Interactive Input\n",
        "# Allow users to input their own questions dynamically\n",
        "while True:\n",
        "    user_input = input(\"Ask a question (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "    print(\"AI Answer:\", ask_question(user_input))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4mzd9GOuYTz"
      },
      "outputs": [],
      "source": [
        "#  Add Memory (Contextual Conversations)\n",
        "\n",
        "def chat_with_memory():\n",
        "    conversation_history = []  # Stores past interactions\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Ask a question (or type 'exit' to quit): \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        # Add user input to history\n",
        "        conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Generate AI response with context\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=conversation_history,  # Pass full conversation\n",
        "            temperature=0.7,\n",
        "            max_tokens=100\n",
        "        )\n",
        "\n",
        "        ai_answer = response.choices[0].message.content.strip()\n",
        "        print(\"\\nAI Answer:\", ai_answer, \"\\n\")\n",
        "\n",
        "        # Add AI response to history\n",
        "        conversation_history.append({\"role\": \"assistant\", \"content\": ai_answer})\n",
        "\n",
        "# Start chatbot with memory\n",
        "chat_with_memory()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qQK3BjKvXu7",
        "outputId": "1192d856-cdab-411f-84e1-5e233010a159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You: do you remember why i like winter?\n",
            "\n",
            "AI: Yes, you like winter because of the peaceful and serene\n",
            "atmosphere, the beauty of snowfall, and the opportunities\n",
            "for cozy indoor activities like reading by the fireplace. \n",
            "\n",
            "\n",
            "You: Nah. because i like skiing haha. \n",
            "\n",
            "AI: Ah, my mistake! Skiing is definitely a fantastic winter\n",
            "activity and a great reason to love the season. The thrill\n",
            "of gliding down the slopes and the crisp, cold air are\n",
            "definitely highlights of winter for skiing enthusiasts. \n",
            "\n",
            "\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "import textwrap\n",
        "\n",
        "def format_output(text, width=60):\n",
        "    return \"\\n\".join(textwrap.wrap(text, width))\n",
        "\n",
        "def chat_with_memory():\n",
        "    conversation_history = []\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=conversation_history,\n",
        "            temperature=0.7,\n",
        "            max_tokens=100\n",
        "        )\n",
        "\n",
        "        ai_answer = response.choices[0].message.content.strip()\n",
        "        print(\"\\nAI:\", format_output(ai_answer), \"\\n\")\n",
        "\n",
        "        conversation_history.append({\"role\": \"assistant\", \"content\": ai_answer})\n",
        "\n",
        "# Start chatbot with memory and improved formatting\n",
        "chat_with_memory()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd88jdFnxHP_",
        "outputId": "dac66119-026b-4d51-877f-e4d3d8af8034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "AI: The time it takes to learn the skills needed to become an AI\n",
            "engineer can vary depending on your prior experience and\n",
            "level of dedication. If you already have software\n",
            "development skills, you may have a good foundation to build\n",
            "upon.  Typically, it can take several months to a year to\n",
            "learn the necessary skills for AI engineering, including\n",
            "understanding machine learning algorithms, data analysis,\n",
            "programming languages such as Python, and familiarity with\n",
            "AI tools and technologies.  To accelerate your learning\n",
            "process, you could consider taking online courses, attending\n",
            "workshops, working on personal projects, and seeking\n",
            "mentorship from experienced AI engineers. With consistent\n",
            "practice and dedication, you could potentially become\n",
            "proficient in AI engineering in a shorter amount of time. \n",
            "\n",
            "\n",
            "AI: While it is difficult to provide an exact timeframe as\n",
            "learning is a highly individualized process, with your\n",
            "background in software development, you could potentially\n",
            "learn the skills for AI engineering in as little as six\n",
            "months to a year. This timeline would involve consistent\n",
            "study, practice, and hands-on projects to solidify your\n",
            "understanding of AI concepts and technologies.  However,\n",
            "it's important to keep in mind that mastering AI engineering\n",
            "is a continuous journey, and you may need to continually\n",
            "update your skills and stay informed about the latest\n",
            "developments in the field. It's recommended to set realistic\n",
            "goals and milestones for your learning journey to track your\n",
            "progress and stay motivated. \n",
            "\n",
            "\n",
            "AI: As an AI language model, I don‚Äôt have access to external\n",
            "websites or databases, so I cannot provide real-time\n",
            "information or verify the specifics of the products or\n",
            "services of companies like Deepseek. My responses are\n",
            "generated based on a mixture of licensed data, data created\n",
            "by human trainers, and publicly available data.  If Deepseek\n",
            "is a product that uses AI technology similar to ChatGPT, it\n",
            "is possible that they may have taken inspiration from or\n",
            "utilized similar principles. However, the level of\n",
            "improvement or differentiation in their product would depend\n",
            "on their specific approach, resources, and goals. It‚Äôs\n",
            "essential to conduct thorough research or reach out to\n",
            "Deepseek directly for accurate and up-to-date information on\n",
            "their offerings and how they may differ from ChatGPT. \n",
            "\n",
            "\n",
            "You: exit\n"
          ]
        }
      ],
      "source": [
        "def chat_with_limited_memory():\n",
        "    conversation_history = []  # Stores chat history\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Keep only the last 10 messages\n",
        "        conversation_history = conversation_history[-10:]\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=conversation_history,\n",
        "            temperature=0.9,\n",
        "            max_tokens=200\n",
        "        )\n",
        "\n",
        "        ai_answer = response.choices[0].message.content.strip()\n",
        "        print(\"\\nAI:\", format_output(ai_answer), \"\\n\")\n",
        "\n",
        "        conversation_history.append({\"role\": \"assistant\", \"content\": ai_answer})\n",
        "\n",
        "# Start chatbot with limited memory\n",
        "chat_with_limited_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOgFVxVoPDzm"
      },
      "source": [
        "The below steps are to build a web-based chatbot. FastAPI is used in the backend. React for the frontend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hZaTme-tNjb"
      },
      "outputs": [],
      "source": [
        "# Step 2: Write a Basic FastAPI App\n",
        "# Now, create a Python script that defines the FastAPI chatbot:\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import openai\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = \"\"\n",
        "\n",
        "# Define request model\n",
        "class ChatRequest(BaseModel):\n",
        "    message: str\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "def chat(request: ChatRequest):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": request.message}]\n",
        "    )\n",
        "    return {\"response\": response.choices[0].message[\"content\"].strip()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI4rCZP9JDzo"
      },
      "outputs": [],
      "source": [
        "# Step 3: Run FastAPI in Colab with ngrok\n",
        "# Since Colab doesn‚Äôt support direct API hosting, we use ngrok to expose it.\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "# Authenticate ngrok (only needed once)\n",
        "ngrok.set_auth_token(\"YOUR_NGROK_AUTH_TOKEN\")\n",
        "\n",
        "# Start FastAPI in a separate thread\n",
        "def run_api():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n",
        "\n",
        "threading.Thread(target=run_api, daemon=True).start()\n",
        "\n",
        "# Create a public URL for the API\n",
        "public_url = ngrok.connect(8080)\n",
        "print(f\"Public URL: {public_url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHIae3OT1Agv",
        "outputId": "58997222-3fa3-49e0-c542-9ac7626d8a45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:\n",
            " kill [options] <pid> [...]\n",
            "\n",
            "Options:\n",
            " <pid> [...]            send signal to every <pid> listed\n",
            " -<signal>, -s, --signal <signal>\n",
            "                        specify the <signal> to be sent\n",
            " -q, --queue <value>    integer value to be sent with the signal\n",
            " -l, --list=[<signal>]  list all signal names, or convert one to a name\n",
            " -L, --table            list all signal names in a nice table\n",
            "\n",
            " -h, --help     display this help and exit\n",
            " -V, --version  output version information and exit\n",
            "\n",
            "For more details see kill(1).\n",
            "\n",
            "Usage:\n",
            " kill [options] <pid> [...]\n",
            "\n",
            "Options:\n",
            " <pid> [...]            send signal to every <pid> listed\n",
            " -<signal>, -s, --signal <signal>\n",
            "                        specify the <signal> to be sent\n",
            " -q, --queue <value>    integer value to be sent with the signal\n",
            " -l, --list=[<signal>]  list all signal names, or convert one to a name\n",
            " -L, --table            list all signal names in a nice table\n",
            "\n",
            " -h, --help     display this help and exit\n",
            " -V, --version  output version information and exit\n",
            "\n",
            "For more details see kill(1).\n"
          ]
        }
      ],
      "source": [
        "!lsof -t -i:8000 | xargs kill -9\n",
        "!lsof -t -i:8501 | xargs kill -9\n",
        "!lsof -t -i:8080 | xargs kill -9\n",
        "# this cell is just to kill the port numbers already in use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMrsX2eB1PTq"
      },
      "outputs": [],
      "source": [
        "!pkill -f ngrok # this is to kill the active ngrok tunnels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAuiuYQCLKAn"
      },
      "source": [
        "# **The below is the code for a web-based chatbox**\n",
        "4 Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjRHEv0ysrYb",
        "outputId": "3a6a1962-0185-4844-f65a-93bee9de513e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.68.2)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, pyngrok, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 pyngrok-7.2.3 starlette-0.46.1 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Dependencies\n",
        "# Run the following command in Colab to install FastAPI, Uvicorn, and OpenAI:\n",
        "!pip install fastapi uvicorn openai pyngrok requests nest_asyncio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgEn8Ac6RG-Z"
      },
      "source": [
        "**Extend the FastAPI chatbot by adding a proper Q&A endpoint where users can send a question and receive an AI-generated response. üöÄ**\n",
        "\n",
        "üìå Steps:\n",
        "Modify FastAPI to handle AI chatbot responses\n",
        "\n",
        "Deploy with Ngrok so it works online\n",
        "\n",
        "(Optional) Add a simple frontend later\n",
        "\n",
        "Before running the below 2 cells if there was error, make sure the port is not in use by running the above 3 cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLsfChmU1uqd"
      },
      "outputs": [],
      "source": [
        "# this is the working code combining the above steps to run ngrok\n",
        "from fastapi import FastAPI\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import threading\n",
        "\n",
        "# Step 1: Define your FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    return {\"message\": \"Hello from FastAPI!\"}\n",
        "\n",
        "# Step 2: Set ngrok auth token (replace with your actual token) # token from my ngrok account logged in via kun.q.tian\n",
        "ngrok.set_auth_token(\"Your ngrok auth token\")\n",
        "\n",
        "# Step 3: Choose a port (e.g., 8501)\n",
        "PORT = 8501\n",
        "\n",
        "# Step 4: Start FastAPI server in a separate thread\n",
        "def run_api():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT)\n",
        "\n",
        "threading.Thread(target=run_api, daemon=True).start()\n",
        "\n",
        "# Step 5: Start ngrok tunnel\n",
        "public_url = ngrok.connect(PORT)\n",
        "print(f\"Public URL: {public_url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6HjxoHzjM4R"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Replace with your actual Ngrok public URL\n",
        "url =  \"https://ac1a-35-231-68-131.ngrok-free.app/\"\n",
        "\n",
        "# Define the input question\n",
        "data = {\"question\": \"What is AI?\"}\n",
        "\n",
        "# Send the POST request\n",
        "response = requests.post(url, json=data)\n",
        "\n",
        "# Print the chatbot's response\n",
        "print(response.json())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PplUDPdRRGtO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMeH-9zQvueF",
        "outputId": "76e1d827-38ba-4428-de94-0f919e22aa83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken <your-authtoken>\n",
        "# no need to run this because the below cell connect ngrok with auth token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwe0h0QX7cKm",
        "outputId": "e5d30c95-6c93-4d8c-dbd4-3ddf06e6608c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://30ce-34-16-231-83.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "# Step 2. after restart runtime and install all dependencies, connect to server via ngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set Ngrok authentication token (replace with your token)\n",
        "ngrok.set_auth_token(\"your ngrok token\")  # Make sure this is your actual Ngrok auth token\n",
        "# Connect to the desired port (8501)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Public URL: {public_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0GQRID2Z4fN",
        "outputId": "1942c117-c141-425a-9b94-6a73b8fb87bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.68.2\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "print(openai.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZjzwVxDNM3N"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vor5-2H-NECn"
      },
      "source": [
        "\n",
        "# **The below code need Fixes as in the next cell:**\n",
        "\n",
        "‚úÖ Replaced user_input with request.question.\n",
        "\n",
        "‚úÖ Used the correct way to extract the response: response.choices[0].message.\n",
        "content.\n",
        "\n",
        "‚úÖ Ensured the OpenAI API is called properly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nevl0ZJDbsSU",
        "outputId": "731c9066-a2a0-4dc7-a19c-74e927e8b7ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://047d-34-16-231-83.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [701]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8501 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET /chat HTTP/1.1\" 405 Method Not Allowed\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"POST /chat HTTP/1.1\" 500 Internal Server Error\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/applications.py\", line 1054, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/applications.py\", line 112, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 714, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 734, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 288, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 76, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/starlette/routing.py\", line 73, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 301, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-12-08d084843bcb>\", line 29, in chat\n",
            "    messages=[{\"role\": \"user\", \"content\": user_input}]\n",
            "                                          ^^^^^^^^^^\n",
            "NameError: name 'user_input' is not defined\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [701]\n"
          ]
        }
      ],
      "source": [
        "# this version is not working somehow\n",
        "\n",
        "\n",
        "\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from pyngrok import ngrok\n",
        "# import os\n",
        "\n",
        "# Apply the async fix\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Set OpenAI API Key\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Define request model\n",
        "class ChatRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "# Define chatbot endpoint\n",
        "@app.post(\"/chat\")\n",
        "async def chat(request: ChatRequest):\n",
        "    client = openai.OpenAI(api_key=\"\")  # Ensure you have set your API key in the environment\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[{\"role\": \"user\", \"content\": user_input}]\n",
        ")\n",
        "\n",
        "    # response = openai.ChatCompletion.create(\n",
        "    #     model=\"gpt-3.5-turbo\",\n",
        "    #     messages=[{\"role\": \"user\", \"content\": request.question}]\n",
        "    # )\n",
        "    return {\"answer\": response.choices[0].message[\"content\"]}\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Welcome to the AI Chatbot API! Use /chat to interact.\"}\n",
        "\n",
        "\n",
        "# Start Ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Run the FastAPI server inside Colab\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8501)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N56EPoe2KzSO"
      },
      "source": [
        "Step 3. The below is the actual working code for implementing a web-based chatbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIqzvcdcgq27",
        "outputId": "7c16270a-90ba-47f9-9f47-46b0aba1da31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://51ef-34-16-231-83.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [701]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8501 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET /chat HTTP/1.1\" 405 Method Not Allowed\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"POST /chat HTTP/1.1\" 200 OK\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET /chat HTTP/1.1\" 405 Method Not Allowed\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET /chat HTTP/1.1\" 405 Method Not Allowed\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"GET /chat HTTP/1.1\" 405 Method Not Allowed\n",
            "INFO:     2600:1016:a005:4b2b:6ce3:4c79:e6d:e0b3:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import openai\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Apply the async fix\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Define request model\n",
        "class ChatRequest(BaseModel):\n",
        "    question: str\n",
        "\n",
        "# Define chatbot endpoint\n",
        "@app.post(\"/chat\")\n",
        "async def chat(request: ChatRequest):\n",
        "    client = openai.OpenAI(api_key=\"\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": request.question}]\n",
        "    )\n",
        "\n",
        "    return {\"answer\": response.choices[0].message.content}  # Fix response extraction\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Welcome to the AI Chatbot API! Use /chat to interact.\"}\n",
        "\n",
        "# Start Ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Run the FastAPI server inside Colab\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8501)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Opr3s76EV0Y"
      },
      "source": [
        "**Step 4: Testing /docs for POST request**\n",
        "\n",
        "Go to the FastAPI interactive documentation at /docs (e.g., https://<your-ngrok-url>/docs) and use the UI there to try sending a POST request directly through the browser:\n",
        "\n",
        "Open the URL https://<your-ngrok-url>/docs.\n",
        "You will see a /chat endpoint listed in the Swagger UI.\n",
        "Click on /chat, and you will see an option to try it out.\n",
        "Enter a question (e.g., \"What is AI?\") and click \"Execute.\"\n",
        "Check the response in the UI. It should give you a response from OpenAI, or an error message if something is wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKEtt3pqzBob"
      },
      "outputs": [],
      "source": [
        "# no need to run this cell - was for debugging only\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"https://8131-35-231-68-131.ngrok-free.app/chat\"  # Update this!\n",
        "\n",
        "response = requests.options(url)  # Check allowed methods\n",
        "print(response.status_code)\n",
        "print(response.headers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THggF2EBjyHz"
      },
      "outputs": [],
      "source": [
        "# this cell shall be run at the same time while uvicorn.run cell is running successfully. but I could not get it work.\n",
        "# Instead test it in the UI via url/docs link as the above step 4\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"https://51ef-34-16-231-83.ngrok-free.app/chat\"  # Replace with your ngrok URL\n",
        "data = {\"question\": \"Hello, how are you?\"}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "print(response.json())  # Should return the chatbot's answer\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
